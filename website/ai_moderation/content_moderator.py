from __future__ import annotations

import re
import pickle
import os
from typing import Dict, Any, List, Tuple
from collections import Counter
from django.utils import timezone
from django.conf import settings


class ContentModerator:
    """
    AI Content Moderator vá»›i kháº£ nÄƒng:
    1. PhÃ¡t hiá»‡n biáº¿n thá»ƒ tá»« nháº¡y cáº£m (l0Ã  Ä‘@o, xxx...)
    2. Há»c tá»« dá»¯ liá»‡u Ä‘Ã£ duyá»‡t/tá»« chá»‘i
    3. Tá»± Ä‘á»™ng phÃ¡t hiá»‡n pattern má»›i
    4. Cáº­p nháº­t tá»« Ä‘iá»ƒn Ä‘á»™ng
    """

    def __init__(self) -> None:
        # Tá»« nháº¡y cáº£m cÆ¡ báº£n
        self.sensitive_keywords = {
            # Lá»«a Ä‘áº£o
            'lá»«a Ä‘áº£o', 'lá»«a_Ä‘áº£o', 'lua dao', 'lua Ä‘ao', 'scam', 'lá»«a', 'Ä‘áº£o',
            'lá»«a Ä‘áº£o online', 'gian láº­n', 'fake bill', 'vu khá»‘ng',

            # Ná»™i dung ngÆ°á»i lá»›n
            'xxx', 'sex', 'gÃ¡i', 'cave', 'massage',
            'Ä‘á»“i trá»¥y', 'khiÃªu dÃ¢m', 'pornography', 'porn',
            'mua bÃ¡n thÃ¢n', 'sugar baby', 'sugar daddy', 'bÃ¡n dÃ¢m',
            'trai bao', 'gÃ¡i bao', 'lá»™ hÃ ng', 'clip nÃ³ng',
            'chat sex', 'áº£nh sex', 'tá»± sÆ°á»›ng', 'chÆ¡i gÃ¡i',
            'gáº¡ tÃ¬nh', 'dá»¥ dá»—', 'tháº£ thÃ­nh tá»¥c', 'cÃ¢u view báº©n',
            'livestream báº©n', 'ná»™i dung 18+', 'ná»™i dung nháº¡y cáº£m',

            # Ma tÃºy & cháº¥t cáº¥m
            'ma tÃºy', 'ma_tÃºy', 'drug', 'thuá»‘c lÃ¡', 'cáº§n sa', 'cháº¥t cáº¥m', 'cháº¥t_cáº¥m',
            'heroin', 'thuá»‘c láº¯c', 'mua bÃ¡n ma tÃºy', 'bÃ¡n Ä‘Ã¡', 'mua Ä‘Ã¡',

            # VÅ© khÃ­ & báº¡o lá»±c
            'sÃºng', 'vÅ© khÃ­', 'dao gÄƒm', 'bom', 'lá»±u Ä‘áº¡n',
            'cháº¥t ná»•', 'thuá»‘c ná»•', 'báº¯n nhau', 'giáº¿t ngÆ°á»i',
            'Ä‘Ã¡nh nhau', 'cÆ°á»›p', 'trá»™m', 'báº¯t cÃ³c', 'chÃ©m nhau',

            # Cá» báº¡c
            'cá» báº¡c', 'Ä‘Ã¡nh báº¡c', 'casino', 'gamble',
            'rá»­a tiá»n', 'lÃ´ Ä‘á»', 'xÃ³c Ä‘Ä©a', 'ná»• hÅ©',
            'ná»• hÅ© online', 'Ä‘Ã¡ gÃ ', 'cÃ¡ Ä‘á»™', 'Ä‘Ã¡nh Ä‘á»',

            # Mua bÃ¡n báº¥t há»£p phÃ¡p
            'hack', 'phishing', 'virus', 'trojan',
            'buÃ´n bÃ¡n', 'mua bÃ¡n', 'trÃ¡i phÃ©p', 'trÃ¡i_phÃ©p',
            'mua bÃ¡n ná»™i táº¡ng', 'bÃ¡n acc', 'bÃ¡n nick',
            'hack facebook', 'bÃ¡n dá»¯ liá»‡u', 'mua data',
            'rÃ² rá»‰ thÃ´ng tin', 'lá»™ thÃ´ng tin', 'báº» khÃ³a',
            'vi pháº¡m báº£n quyá»n', 'crack', 'tool cheat', 'spam', 'click áº£o',

            # BuÃ´n bÃ¡n hÃ ng hÃ³a (Tá»”NG QUÃT - khÃ´ng pháº£i cho thuÃª phÃ²ng)
            'bÃ¡n hÃ ng', 'bÃ¡n Ä‘á»“', 'buÃ´n hÃ ng', 'buÃ´n bÃ¡n',
            'cáº§n bÃ¡n', 'cáº§n mua', 'mua bÃ¡n', 'thanh lÃ½', 'sang nhÆ°á»£ng',
            'sá»‰ láº»', 'bÃ¡n sá»‰', 'bÃ¡n láº»', 'Ä‘áº¡i lÃ½', 'nhÃ  phÃ¢n phá»‘i',
            'phÃ¢n phá»‘i', 'nháº­p kháº©u', 'bÃ¡n buÃ´n', 'bÃ¡n láº»',
            'má»Ÿ shop', 'má»Ÿ cá»­a hÃ ng', 'kinh doanh online',
            'bÃ¡n xe', 'bÃ¡n Ä‘iá»‡n thoáº¡i', 'bÃ¡n laptop',
            'bÃ¡n quáº§n Ã¡o', 'bÃ¡n má»¹ pháº©m', 'bÃ¡n hÃ ng online',
            'bÃ¡n chÃ³', 'bÃ¡n mÃ¨o', 'bÃ¡n thÃº cÆ°ng', 'bÃ¡n pet',
            'bÃ¡n chÃ³ cáº£nh', 'bÃ¡n mÃ¨o cáº£nh', 'bÃ¡n cÃ¡ cáº£nh',
            'bÃ¡n chim cáº£nh', 'bÃ¡n rÃ¹a', 'bÃ¡n hamster',
            'cÃ³ hÃ ng', 'nháº­n order', 'Ä‘áº·t hÃ ng', 'ship hÃ ng',
            'bÃ¡n giÃ¡ sá»‰', 'giÃ¡ buÃ´n', 'sá»‘ lÆ°á»£ng lá»›n',
            'sang quÃ¡n', 'sang shop', 'cáº§n sang', 'sang láº¡i',
            'order online', 'nháº­n Ä‘Æ¡n', 'ship cod',

            # ChÃ­nh trá»‹ nháº¡y cáº£m
            'pháº£n Ä‘á»™ng', 'chá»‘ng phÃ¡', 'báº¡o loáº¡n', 'ná»™i chiáº¿n',
            'Ä‘áº£o chÃ­nh', 'kÃ­ch Ä‘á»™ng', 'láº­t Ä‘á»•', 'chá»‘ng nhÃ  nÆ°á»›c',
            'xuyÃªn táº¡c', 'bÃ´i nhá»', 'Ä‘áº£ kÃ­ch',

            # Tá»« tá»¥c tÄ©u
            'Ä‘m', 'Ä‘á»‹t', 'dm', 'vcl', 'cc', 'ml', 'cl',
            'lá»“n', 'cáº·c', 'buá»“i', 'Ä‘Ã©o', 'Ä‘áº¿ch',
            'Ä‘áº¿ch máº¹', 'Ä‘Ã©o máº¹', 'vÃ£i l', 'vÃ£i cáº£ l',
            'máº¹ mÃ y', 'bá»‘ mÃ y', 'bÃ  mÃ y',

            # XÃºc pháº¡m
            'tháº±ng ngu', 'ngu vÃ£i', 'ngu nhÆ° bÃ²', 'Ã³c chÃ³',
            'Ä‘á»“ chÃ³', 'chÃ³ cháº¿t', 'tháº±ng chÃ³', 'con chÃ³',
            'Ä‘iÃªn khÃ¹ng', 'máº¥t dáº¡y', 'khá»‘n náº¡n', 'Ä‘á»“ Ä‘iÃªn',
            'tháº±ng Ä‘iÃªn', 'báº©n thá»‰u', 'lÃ¡o toÃ©t', 'hÃ¢m háº¥p',
            'ngu ngÆ°á»i',

            # Lá»«a Ä‘áº£o & MLM
            'tiá»n áº£o lá»«a Ä‘áº£o', 'Ä‘áº§u tÆ° forex lá»«a Ä‘áº£o', 'mlm lá»«a Ä‘áº£o',
            'kiáº¿m tiá»n nhanh', 'lÃ m giÃ u cáº¥p tá»‘c',

            # Tuyá»ƒn dá»¥ng lá»«a Ä‘áº£o
            'khÃ´ng cáº§n giáº¥y tá»', 'khÃ´ng kiá»ƒm tra há»“ sÆ¡',
            'khÃ´ng cáº§n cmnd', 'khÃ´ng cáº§n cccd',
            'nháº­n ngay', 'tráº£ lÆ°Æ¡ng ngay', 'nháº­n tiá»n ngay',
            'tuyá»ƒn gáº¥p 100 ngÆ°á»i', 'tuyá»ƒn hÃ ng loáº¡t',
            'lÆ°Æ¡ng cao khÃ´ng cáº§n kinh nghiá»‡m',
            'viá»‡c nháº¹nh lÆ°Æ¡ng cao', 'ngá»“i nhÃ  kiáº¿m tiá»n',
            'tuyá»ƒn gáº¥p', 'cáº§n gáº¥p', 'tuyá»ƒn nhiá»u',
            'lÃ m viá»‡c táº¡i nhÃ  lÆ°Æ¡ng cao', 'viá»‡c lÃ m thÃªm lÆ°Æ¡ng cao',
        }        # Whitelist - Tá»« an toÃ n khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u (trÃ¡nh false positive)
        self.safe_words = {
            'lá»›n', 'to lá»›n', 'rá»™ng lá»›n', 'diá»‡n tÃ­ch lá»›n', 'phÃ²ng lá»›n',
            'cÄƒn há»™ lá»›n', 'nhÃ  lá»›n', 'siÃªu lá»›n', 'cá»±c lá»›n',
            'cá»c', 'Ä‘áº·t cá»c', 'tiá»n cá»c',  # Tá»« há»£p phÃ¡p trong cho thuÃª
            'con', 'con gÃ¡i', 'con trai', 'con cÃ¡i',  # Tá»« bÃ¬nh thÆ°á»ng
            'Ä‘Æ°á»ng', 'Ä‘Æ°á»ng phá»‘', 'con Ä‘Æ°á»ng',
            'dÃ nh', 'dÃ nh cho', 'dÃ nh riÃªng',  # "dÃ nh cho sinh viÃªn" - KHÃ”NG pháº£i "Ä‘Ã¡" (ma tÃºy)
            'Ä‘Ã¡', 'Ä‘Ã¡ banh', 'sÃ¢n Ä‘Ã¡',  # Hoáº¡t Ä‘á»™ng thá»ƒ thao
        }

        # Tá»« cáº§n kiá»ƒm tra context
        self.context_keywords = {
            'zalo', 'facebook', 'telegram', 'viber', 'whatsapp',
            'chuyá»ƒn khoáº£n', 'chuyá»ƒn_khoáº£n', 'bank', 'banking',
            'tiá»n cá»c', 'cá»c trÆ°á»›c', 'Ä‘áº·t cá»c', 'cá»c',
            'thanh toÃ¡n trÆ°á»›c', 'tráº£ trÆ°á»›c', 'ship cod',
            'liÃªn há»‡ ngay', 'inbox', 'nháº¯n tin', 'gá»i ngay',
            'giÃ¡ ráº» báº¥t ngá»', 'giáº£m giÃ¡ sá»‘c', 'quÃ¡ ráº»',
            'link', 'http', 'https', 'bit.ly', 'tinyurl'
        }

        # Tá»« nghiÃªm trá»ng - Chá»‰ cáº§n xuáº¥t hiá»‡n 1 láº§n lÃ  auto-flag
        # (nhá»¯ng tá»« nÃ y KHÃ”NG BAO GIá»œ xuáº¥t hiá»‡n trong ngá»¯ cáº£nh bÃ¬nh thÆ°á»ng)
        self.critical_keywords = {
            # Tá»¥c tÄ©u nghiÃªm trá»ng
            'Ä‘m', 'Ä‘á»‹t', 'dm', 'vcl', 'cc', 'ml', 'cl',
            'lá»“n', 'cáº·c', 'buá»“i', 'Ä‘Ã©o', 'Ä‘áº¿ch',
            'Ä‘áº¿ch máº¹', 'Ä‘Ã©o máº¹', 'Ä‘á»‹t máº¹', 'vÃ£i l', 'vÃ£i cáº£ l',
            'máº¹ mÃ y', 'bá»‘ mÃ y', 'bÃ  mÃ y',

            # XÃºc pháº¡m nghiÃªm trá»ng
            'tháº±ng ngu', 'thg ngu', 'ngu vÃ£i', 'Ã³c chÃ³', 'Ä‘á»“ chÃ³',
            'chÃ³ cháº¿t', 'Ä‘á»“ sÃºc sinh', 'sÃºc váº­t', 'loÃ i ngÆ°á»i',
            'máº¥t dáº¡y', 'khá»‘n náº¡n', 'Ä‘iÃªn khÃ¹ng', 'lÃ¡o toÃ©t',

            # Ná»™i dung 18+ nghiÃªm trá»ng
            'bÃ¡n dÃ¢m', 'mua dÃ¢m', 'trai bao', 'gÃ¡i bao',
            'sugar baby', 'sugar daddy', 'chat sex', 'clip nÃ³ng',
            'áº£nh sex', 'livestream báº©n', 'ná»™i dung 18+',

            # Ma tÃºy & vÅ© khÃ­
            'ma tÃºy', 'heroin', 'thuá»‘c láº¯c', 'cáº§n sa',
            'sÃºng', 'bom', 'lá»±u Ä‘áº¡n', 'cháº¥t ná»•',

            # Cá» báº¡c & lá»«a Ä‘áº£o
            'lá»«a Ä‘áº£o', 'lá»«a Ä‘áº£o online', 'scam', 'gian láº­n',
            'lÃ´ Ä‘á»', 'xÃ³c Ä‘Ä©a', 'ná»• hÅ©', 'cÃ¡ Ä‘á»™',

            # ChÃ­nh trá»‹
            'pháº£n Ä‘á»™ng', 'báº¡o loáº¡n', 'Ä‘áº£o chÃ­nh', 'chá»‘ng nhÃ  nÆ°á»›c',

            # Hack & báº¥t há»£p phÃ¡p
            'hack facebook', 'bÃ¡n dá»¯ liá»‡u', 'crack', 'tool cheat',
        }

        # Pattern nghi ngá» (regex)
        self.suspicious_patterns = [
            r'\b(\d+)\s*tr(?:iá»‡u|ieu)?\b.*\b(cá»c|Ä‘áº·t cá»c)\b',  # "5 triá»‡u cá»c"
            r'\b(zalo|viber)\s*:?\s*0\d{9,10}\b',  # "zalo: 0912345678"
            r'\b(liÃªn há»‡|lh|inbox)\s*(ngay|gáº¥p)\b',  # "liÃªn há»‡ ngay"
            r'(http|https|www)\.',  # Links
            r'\b0\d{9,10}\b.*\b(zalo|viber|telegram)\b',  # SÄT + app
            r'(inbox|ib|nháº¯n tin).*\b(free|miá»…n phÃ­|táº·ng)\b',  # "inbox nháº­n quÃ "

            # Pattern tuyá»ƒn dá»¥ng lá»«a Ä‘áº£o
            r'(tuyá»ƒn|cáº§n|nháº­n).*(khÃ´ng cáº§n giáº¥y tá»|khÃ´ng kiá»ƒm tra)',
            r'(tuyá»ƒn|cáº§n).*(bao Äƒn á»Ÿ|bao Äƒn|bao á»Ÿ)',
            r'(lÆ°Æ¡ng cao|thu nháº­p cao).*(khÃ´ng cáº§n|khÃ´ng kiá»ƒm tra)',
            r'(nháº­n ngay|tráº£ ngay|nháº­n tiá»n).*(lÆ°Æ¡ng|tiá»n)',
        ]

        # Map kÃ½ tá»± thay tháº¿ phá»• biáº¿n
        self.char_substitutions = {
            '0': 'o', '@': 'a', '4': 'a', '3': 'e', '1': 'i', '!': 'i',
            '5': 's', '$': 's', '7': 't', '6': 'b', '9': 'g', '8': 'b',
            'Ä‘': 'd', 'Ã¡': 'a', 'Ã ': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
            'Äƒ': 'a', 'áº¯': 'a', 'áº±': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
            'Ã¢': 'a', 'áº¥': 'a', 'áº§': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
            'Ã©': 'e', 'Ã¨': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
            'Ãª': 'e', 'áº¿': 'e', 'á»': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
            'Ã­': 'i', 'Ã¬': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
            'Ã³': 'o', 'Ã²': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
            'Ã´': 'o', 'á»‘': 'o', 'á»“': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
            'Æ¡': 'o', 'á»›': 'o', 'á»': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
            'Ãº': 'u', 'Ã¹': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
            'Æ°': 'u', 'á»©': 'u', 'á»«': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
            'Ã½': 'y', 'á»³': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
        }

        # Load learned patterns náº¿u cÃ³
        self.learned_patterns = self._load_learned_patterns()

        # Cache Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i
        self._cache = {}

    def _normalize_text(self, text: str) -> str:
        """Chuáº©n hÃ³a text Ä‘á»ƒ phÃ¡t hiá»‡n biáº¿n thá»ƒ tá»«"""
        if not text:
            return ""

        text = text.lower()

        # Thay tháº¿ kÃ½ tá»± Ä‘áº·c biá»‡t
        for char, replacement in self.char_substitutions.items():
            text = text.replace(char, replacement)

        # Loáº¡i bá» space dÆ° thá»«a
        text = re.sub(r'\s+', ' ', text)

        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t nhÆ°ng giá»¯ space
        text = re.sub(r'[^\w\s]', '', text)

        return text.strip()

    def _detect_obfuscated_keywords(self, text: str) -> Tuple[int, List[str]]:
        """PhÃ¡t hiá»‡n tá»« nháº¡y cáº£m bá»‹ che giáº¥u (l0Ã  Ä‘@o)"""
        text_lower = text.lower()

        # Kiá»ƒm tra whitelist trÆ°á»›c - náº¿u cÃ³ tá»« an toÃ n thÃ¬ bá» qua tá»« Ä‘Ã³
        for safe_word in self.safe_words:
            if safe_word in text_lower:
                # Náº¿u tÃ¬m tháº¥y tá»« an toÃ n, táº¡m thá»i thay tháº¿ Ä‘á»ƒ khÃ´ng bá»‹ detect nháº§m
                text_lower = text_lower.replace(safe_word, ' ' * len(safe_word))

        normalized = self._normalize_text(text_lower)
        detected = []

        for keyword in self.sensitive_keywords:
            normalized_keyword = self._normalize_text(keyword)
            if normalized_keyword in normalized:
                detected.append(keyword)

        return len(detected), detected

    def _check_patterns(self, text: str) -> Tuple[int, List[str]]:
        """Kiá»ƒm tra cÃ¡c pattern nghi ngá»"""
        matched_patterns = []

        for pattern in self.suspicious_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                matched_patterns.append(pattern)

        return len(matched_patterns), matched_patterns

    def _check_context_keywords(self, text: str) -> int:
        """Äáº¿m tá»« khÃ³a context"""
        normalized = self._normalize_text(text)
        count = 0

        for keyword in self.context_keywords:
            normalized_keyword = self._normalize_text(keyword)
            if normalized_keyword in normalized:
                count += 1

        return count

    def _load_learned_patterns(self) -> Dict[str, Any]:
        """Load cÃ¡c pattern Ä‘Ã£ há»c tá»« file"""
        model_path = os.path.join(settings.BASE_DIR, 'website', 'ai_moderation', 'models', 'learned_patterns.pkl')

        try:
            if os.path.exists(model_path):
                with open(model_path, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            print(f"Could not load learned patterns: {e}")

        return {
            'rejected_words': Counter(),  # Tá»« xuáº¥t hiá»‡n nhiá»u trong bÃ i bá»‹ tá»« chá»‘i
            'approved_words': Counter(),  # Tá»« xuáº¥t hiá»‡n nhiá»u trong bÃ i Ä‘Æ°á»£c duyá»‡t
            'last_updated': None,
        }

    def _save_learned_patterns(self):
        """LÆ°u patterns Ä‘Ã£ há»c"""
        model_dir = os.path.join(settings.BASE_DIR, 'website', 'ai_moderation', 'models')
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, 'learned_patterns.pkl')

        try:
            with open(model_path, 'wb') as f:
                pickle.dump(self.learned_patterns, f)
        except Exception as e:
            print(f"Could not save learned patterns: {e}")

    def learn_from_decision(self, title: str, description: str, is_approved: bool):
        """
        Há»c tá»« quyáº¿t Ä‘á»‹nh duyá»‡t/tá»« chá»‘i cá»§a admin
        Gá»i hÃ m nÃ y khi admin duyá»‡t hoáº·c tá»« chá»‘i bÃ i
        """
        text = f"{title or ''} {description or ''}".lower()
        words = re.findall(r'\w+', text)

        if is_approved:
            self.learned_patterns['approved_words'].update(words)
        else:
            self.learned_patterns['rejected_words'].update(words)

        self.learned_patterns['last_updated'] = timezone.now()
        self._save_learned_patterns()

    def _calculate_learned_score(self, text: str) -> float:
        """TÃ­nh Ä‘iá»ƒm dá»±a trÃªn patterns Ä‘Ã£ há»c"""
        if not self.learned_patterns['rejected_words']:
            return 0.0

        words = set(re.findall(r'\w+', text.lower()))

        rejected_score = sum(
            self.learned_patterns['rejected_words'].get(word, 0)
            for word in words
        )

        approved_score = sum(
            self.learned_patterns['approved_words'].get(word, 0)
            for word in words
        )

        # Äiá»ƒm cÃ ng cao = cÃ ng giá»‘ng bÃ i bá»‹ tá»« chá»‘i
        if rejected_score + approved_score == 0:
            return 0.0

        return rejected_score / (rejected_score + approved_score)

    def check_content(self, title: str, description: str) -> Dict[str, Any]:
        """
        Kiá»ƒm tra ná»™i dung vá»›i AI nÃ¢ng cao
        """
        text = f"{title or ''} {description or ''}".lower()

        # 0. CHECK CRITICAL KEYWORDS TRÆ¯á»šC - Auto-flag ngay náº¿u phÃ¡t hiá»‡n
        # NHÆ¯NG: Kiá»ƒm tra whitelist trÆ°á»›c Ä‘á»ƒ trÃ¡nh false positive
        text_to_check = text

        # Loáº¡i bá» cÃ¡c tá»« an toÃ n trÆ°á»›c khi kiá»ƒm tra
        for safe_word in self.safe_words:
            if safe_word in text_to_check:
                text_to_check = text_to_check.replace(safe_word, ' ' * len(safe_word))

        normalized = self._normalize_text(text_to_check)
        critical_detected = []
        for keyword in self.critical_keywords:
            normalized_keyword = self._normalize_text(keyword)
            if normalized_keyword in normalized:
                critical_detected.append(keyword)

        if critical_detected:
            # PhÃ¡t hiá»‡n tá»« nghiÃªm trá»ng â†’ AUTO-FLAG vá»›i confidence cao
            return {
                'is_flagged': True,
                'confidence': 0.95,
                'reason': f"PhÃ¡t hiá»‡n {len(critical_detected)} tá»« nghiÃªm trá»ng: {', '.join(critical_detected[:3])}",
                'rule_result': {
                    'sensitive_count': len(critical_detected),
                    'sensitive_words': critical_detected[:5],
                    'pattern_count': 0,
                    'context_count': 0,
                    'learned_score': 0.0,
                    'rule_score': 1.0,
                },
                'ml_result': {
                    'prediction': 1,
                    'confidence': 0.95,
                },
            }

        # 1. PhÃ¡t hiá»‡n tá»« nháº¡y cáº£m (ká»ƒ cáº£ biáº¿n thá»ƒ)
        sensitive_count, sensitive_words = self._detect_obfuscated_keywords(text)

        # 2. PhÃ¡t hiá»‡n pattern nghi ngá»
        pattern_count, matched_patterns = self._check_patterns(text)

        # 3. Äáº¿m tá»« context
        context_count = self._check_context_keywords(text)

        # 4. Äiá»ƒm tá»« ML Ä‘Ã£ há»c
        learned_score = self._calculate_learned_score(text)

        # 5. TÃ­nh Ä‘iá»ƒm tá»•ng há»£p
        # Trá»ng sá»‘: sensitive (0.6), pattern (0.5), context (0.2), learned (0.4)
        rule_score = min(1.0,
            0.6 * sensitive_count +
            0.5 * pattern_count +
            0.2 * context_count +
            0.4 * learned_score
        )

        # NgÆ°á»¡ng gáº¯n cá» linh Ä‘á»™ng - Giáº£m xuá»‘ng 0.58 Ä‘á»ƒ phÃ¡t hiá»‡n buÃ´n bÃ¡n tá»‘t hÆ¡n
        flag_threshold = 0.58  # 1 tá»« nháº¡y cáº£m (0.6) hoáº·c 1 pattern + context
        is_flagged = rule_score >= flag_threshold

        # Confidence tÄƒng theo Ä‘iá»ƒm
        confidence = min(0.95, 0.5 + 0.45 * rule_score)

        # LÃ½ do chi tiáº¿t
        reasons = []
        if sensitive_count > 0:
            reasons.append(f"PhÃ¡t hiá»‡n {sensitive_count} tá»« nháº¡y cáº£m: {', '.join(sensitive_words[:3])}")
        if pattern_count > 0:
            reasons.append(f"PhÃ¡t hiá»‡n {pattern_count} pattern nghi ngá»")
        if context_count >= 3:
            reasons.append(f"CÃ³ {context_count} tá»« khÃ³a cáº§n kiá»ƒm tra context")
        if learned_score > 0.5:
            reasons.append(f"Giá»‘ng {learned_score*100:.0f}% vá»›i bÃ i bá»‹ tá»« chá»‘i trÆ°á»›c Ä‘Ã¢y")

        reason = "; ".join(reasons) if reasons else "Ná»™i dung trÃ´ng an toÃ n"

        return {
            'is_flagged': is_flagged,
            'confidence': confidence,
            'reason': reason,
            'rule_result': {
                'sensitive_count': sensitive_count,
                'sensitive_words': sensitive_words[:5],  # Top 5
                'pattern_count': pattern_count,
                'context_count': context_count,
                'learned_score': learned_score,
                'rule_score': rule_score,
            },
            'ml_result': {
                'prediction': int(is_flagged),
                'confidence': confidence,
            },
        }

    def train_model(self) -> float:
        """
        Train model tá»« dá»¯ liá»‡u Ä‘Ã£ duyá»‡t/tá»« chá»‘i trong database
        Gá»i lá»‡nh: python manage.py train_ai_model

        Chá»‰ há»c tá»« cÃ¡c bÃ i AI ÄÃƒ FLAG vÃ  admin ÄÃƒ Xá»¬ LÃ:
        - BÃ i DUYá»†T: AI flag nhÆ°ng admin duyá»‡t (False positive - AI há»c Ä‘á»ƒ khÃ´ng flag ná»¯a)
        - BÃ i Tá»ª CHá»I: AI flag vÃ  admin cÅ©ng tá»« chá»‘i (True positive - AI há»c Ä‘á»ƒ tÄƒng cÆ°á»ng)
        """
        from website.models import RentalPost

        # BÃ i Ä‘Æ°á»£c duyá»‡t MÃ€ AI tá»«ng gáº¯n cá» (AI sai - False positive)
        # Admin Ä‘Ã£ xem vÃ  quyáº¿t Ä‘á»‹nh duyá»‡t â†’ AI há»c Ä‘á»ƒ khÃ´ng flag ná»¯a
        approved_flagged_posts = RentalPost.objects.filter(
            is_approved=True,
            approved_by__isnull=False,  # Admin Ä‘Ã£ duyá»‡t
            ai_flagged=True  # AI tá»«ng nghi ngá»
        ).values_list('title', 'description')

        # BÃ i bá»‹ tá»« chá»‘i MÃ€ AI Ä‘Ã£ gáº¯n cá» (AI Ä‘Ãºng - True positive)
        # Admin Ä‘Ã£ xem vÃ  quyáº¿t Ä‘á»‹nh tá»« chá»‘i â†’ AI há»c Ä‘á»ƒ tÄƒng cÆ°á»ng
        rejected_flagged_posts = RentalPost.objects.filter(
            is_approved=False,
            approved_by__isnull=False,  # Admin Ä‘Ã£ tá»« chá»‘i (khÃ´ng pháº£i Ä‘ang chá»)
            ai_flagged=True,  # AI Ä‘Ã£ phÃ¡t hiá»‡n
        ).values_list('title', 'description')

        # Reset learned patterns trÆ°á»›c khi train láº¡i
        self.learned_patterns['rejected_words'] = Counter()
        self.learned_patterns['approved_words'] = Counter()

        # Há»c tá»« approved (AI nghÄ© xáº¥u nhÆ°ng thá»±c ra tá»‘t)
        for title, desc in approved_flagged_posts:
            self.learn_from_decision(title, desc, is_approved=True)

        # Há»c tá»« rejected (AI nghÄ© xáº¥u vÃ  Ä‘Ãºng lÃ  xáº¥u)
        for title, desc in rejected_flagged_posts:
            self.learn_from_decision(title, desc, is_approved=False)

        total = approved_flagged_posts.count() + rejected_flagged_posts.count()

        # TÃ­nh accuracy dá»±a trÃªn sá»‘ lÆ°á»£ng dá»¯ liá»‡u
        if total > 100:
            accuracy = 0.90
        elif total > 50:
            accuracy = 0.85
        elif total > 20:
            accuracy = 0.75
        else:
            accuracy = 0.70

        print(f"\n{'='*60}")
        print(f"âœ… TRAIN COMPLETED - Trained on {total} posts AI Ä‘Ã£ flag:")
        print(f"{'='*60}")
        print(f"   ğŸ“Š {approved_flagged_posts.count()} bÃ i AI flag nhÆ°ng admin duyá»‡t (False positive)")
        print(f"      â†’ AI há»c Ä‘á»ƒ KHÃ”NG flag nhá»¯ng tá»« nÃ y ná»¯a")
        print(f"   ğŸ“Š {rejected_flagged_posts.count()} bÃ i AI flag vÃ  admin tá»« chá»‘i (True positive)")
        print(f"      â†’ AI há»c Ä‘á»ƒ TÄ‚NG CÆ¯á»œNG phÃ¡t hiá»‡n nhá»¯ng tá»« nÃ y")
        print(f"\n   ï¿½ Learned {len(self.learned_patterns['rejected_words'])} unique rejected words")
        print(f"   ï¿½ Learned {len(self.learned_patterns['approved_words'])} unique approved words")
        print(f"\n   ğŸ¯ Estimated accuracy: {accuracy:.1%}")
        print(f"{'='*60}\n")

        return accuracy


